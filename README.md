# Localisation de source sur graphe mÃ©trique par mÃ©thode adjointe

## ğŸ“Œ Description
Ce projet implÃ©mente la **localisation de sources** sur des **graphes mÃ©triques 1D/2D** en utilisant :
- une discrÃ©tisation par **diffÃ©rences finies** sur les arÃªtes,
- la **rÃ©solution du problÃ¨me direct**,
- le **calcul de sensibilitÃ©s**,
- la **mÃ©thode adjointe** pour le calcul efficace du gradient,
- une **validation systÃ©matique par diffÃ©rences finies**.

Le cadre est celui dâ€™un **problÃ¨me inverse** gouvernÃ© par une Ã©quation elliptique sur graphe mÃ©trique.

---

## ğŸ§  ModÃ¨le mathÃ©matique

On considÃ¨re le problÃ¨me direct :
\[
A u = g(\varepsilon)
\]

oÃ¹ :
- \( u \) est lâ€™Ã©tat (solution),
- \( g(\varepsilon) \) est une source localisÃ©e (gaussienne) dÃ©pendant du paramÃ¨tre \( \varepsilon \),
- \( A \) est lâ€™opÃ©rateur de diffusion discret sur le graphe.

La fonctionnelle de coÃ»t est :
\[
J(\varepsilon) =
\frac{1}{2} \int (u - u_{\text{data}})^2 \, dx
+ \frac{\varpi}{2} \sum_{\text{bord}} ( \text{flux} - \text{flux}_{\text{data}} )^2
\]

---

## ğŸ¯ Objectifs du code

- Construire des **graphes mÃ©triques** (topologie + gÃ©omÃ©trie)
- RÃ©soudre le **problÃ¨me direct**
- Calculer les **sensibilitÃ©s** \( \partial u / \partial \varepsilon \)
- ImplÃ©menter la **mÃ©thode adjointe**
- Calculer le **gradient du coÃ»t** :
\[
\frac{dJ}{d\varepsilon} = - p^T \frac{\partial g}{\partial \varepsilon}
\]
- Comparer avec les **diffÃ©rences finies** (validation)

---

## ğŸ§© Structure du code

### 1ï¸âƒ£ `MetricGraph`
Classe reprÃ©sentant un **graphe mÃ©trique** :
- sommets internes / de bord,
- arÃªtes avec longueur, diffusion, discrÃ©tisation,
- construction des degrÃ©s de libertÃ© (DDL),
- visualisation 2D du graphe.

### 2ï¸âƒ£ `SourceLocalization`
Classe principale pour le problÃ¨me inverse :
- assemblage du systÃ¨me linÃ©aire,
- rÃ©solution du problÃ¨me direct,
- calcul des sensibilitÃ©s,
- Ã©quation adjointe,
- calcul du gradient,
- fonctionnelle de coÃ»t,
- visualisation des solutions et Ã©tats adjoints.

---

## ğŸ”¬ MÃ©thode adjointe (idÃ©e clÃ©)

Au lieu de calculer une sensibilitÃ© par paramÃ¨tre (coÃ»t Ã©levÃ©), on rÃ©sout :
1. **ProblÃ¨me direct** :  
   \[
   A u = g(\varepsilon)
   \]
2. **ProblÃ¨me adjoint** :  
   \[
   A^T p = -\frac{\partial J}{\partial u}
   \]
3. **Gradient** :
   \[
   \frac{dJ}{d\varepsilon} = -p^T \frac{\partial g}{\partial \varepsilon}
   \]

â¡ï¸ **CoÃ»t indÃ©pendant du nombre de paramÃ¨tres**.

---

## â–¶ï¸ Exemples fournis

Le script principal contient plusieurs cas de test :

- âœ… **Validation 1D** (sensibilitÃ© vs diffÃ©rences finies)
- ğŸ“Š **Ã‰tude de sensibilitÃ© sur graphe 2D**
- ğŸ” **Validation complÃ¨te de la mÃ©thode adjointe**
- ğŸ¨ **Visualisation** :
  - graphe mÃ©trique,
  - solution directe,
  - Ã©tat adjoint,
  - sensibilitÃ©s.

---

## ğŸ–¥ï¸ DÃ©pendances

- Python â‰¥ 3.8
- `numpy`
- `scipy`
- `matplotlib`

Installation :
```bash
pip install numpy scipy matplotlib
