# Localisation de source sur graphe mÃ©trique par mÃ©thode adjointe

## ğŸ“Œ Description

Ce projet traite un **problÃ¨me inverse de localisation de sources** sur des **graphes mÃ©triques en dimension 1 et 2**.

La mÃ©thode repose sur :
- une **discrÃ©tisation par diffÃ©rences finies** sur les arÃªtes,
- la **rÃ©solution du problÃ¨me direct elliptique**,
- le **calcul des sensibilitÃ©s**,
- la **mÃ©thode adjointe** pour le calcul efficace du gradient,
- une **validation par diffÃ©rences finies**.

Le cadre est celui dâ€™un **problÃ¨me inverse gouvernÃ© par une Ã©quation elliptique sur graphe mÃ©trique**.

---

## ğŸ§  ModÃ¨le mathÃ©matique

On considÃ¨re le **problÃ¨me direct** :

**A Â· u = g(Îµ)**

oÃ¹ :
- **u** est lâ€™Ã©tat (solution du problÃ¨me direct),
- **g(Îµ)** est une source localisÃ©e (gaussienne) dÃ©pendant du paramÃ¨tre **Îµ**,
- **A** est lâ€™opÃ©rateur de diffusion discret sur le graphe mÃ©trique.

La **fonctionnelle de coÃ»t** est dÃ©finie par :

**J(Îµ) = 1/2 âˆ« (u âˆ’ u_data)Â² dx  
â€ƒâ€ƒâ€ƒ+ (Ï– / 2) Î£_bord (flux âˆ’ flux_data)Â²**

oÃ¹ **u_data** reprÃ©sente les donnÃ©es de rÃ©fÃ©rence et **Ï– â‰¥ 0** un paramÃ¨tre de pondÃ©ration.

---

## ğŸ¯ Objectifs du code

Le code permet de :
- construire des **graphes mÃ©triques** (topologie et gÃ©omÃ©trie),
- rÃ©soudre le **problÃ¨me direct**,
- calculer les **sensibilitÃ©s** âˆ‚u/âˆ‚Îµ,
- implÃ©menter la **mÃ©thode adjointe**,
- calculer le **gradient de la fonctionnelle de coÃ»t** :

**dJ/dÎµ = âˆ’ páµ€ Â· âˆ‚g/âˆ‚Îµ**

- comparer les rÃ©sultats aux **diffÃ©rences finies** (validation).

---

## ğŸ§© Structure du code

### 1ï¸âƒ£ `MetricGraph`

Classe reprÃ©sentant un **graphe mÃ©trique** :
- sommets internes et sommets de bord,
- arÃªtes avec longueur, coefficient de diffusion et discrÃ©tisation,
- construction des degrÃ©s de libertÃ© (DDL),
- visualisation gÃ©omÃ©trique en 2D.

### 2ï¸âƒ£ `SourceLocalization`

Classe principale dÃ©diÃ©e au **problÃ¨me inverse** :
- assemblage du systÃ¨me linÃ©aire,
- rÃ©solution du problÃ¨me direct,
- calcul des sensibilitÃ©s,
- rÃ©solution de lâ€™Ã©quation adjointe,
- calcul du gradient,
- Ã©valuation de la fonctionnelle de coÃ»t,
- visualisation des solutions directes et adjointes.

---

## ğŸ”¬ MÃ©thode adjointe (principe)

PlutÃ´t que de calculer une sensibilitÃ© par paramÃ¨tre, la mÃ©thode repose sur :

1. **ProblÃ¨me direct**  
   A Â· u = g(Îµ)

2. **ProblÃ¨me adjoint**  
   Aáµ€ Â· p = âˆ’ âˆ‚J/âˆ‚u

3. **Gradient**  
   dJ/dÎµ = âˆ’ páµ€ Â· âˆ‚g/âˆ‚Îµ

ğŸ‘‰ Le coÃ»t de calcul est **indÃ©pendant du nombre de paramÃ¨tres**.

---

## â–¶ï¸ Exemples fournis

Le script principal inclut :
- âœ… **Validation 1D** (sensibilitÃ© vs diffÃ©rences finies),
- ğŸ“Š **Ã‰tude de sensibilitÃ© sur graphe mÃ©trique 2D**,
- ğŸ” **Validation complÃ¨te de la mÃ©thode adjointe**,
- ğŸ¨ **Visualisations** :
  - graphe mÃ©trique,
  - solution directe,
  - Ã©tat adjoint,
  - champs de sensibilitÃ©.

---

## ğŸ–¥ï¸ DÃ©pendances

- Python â‰¥ 3.8
- `numpy`
- `scipy`
- `matplotlib`

Installation :
```bash
pip install numpy scipy matplotlib
